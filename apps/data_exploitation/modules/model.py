import tensorflow as tf


def lr_warmup_cosine_decay(global_step,
                           warmup_steps,
                           hold = 0,
                           total_steps=0,
                           start_lr=0.0,
                           target_lr=1e-3):
    # Convert types
    global_step = tf.cast(global_step, tf.float32)
    warmup_steps = tf.cast(warmup_steps, tf.float32)
    total_steps = tf.cast(total_steps, tf.float32)
    hold = tf.cast(hold, tf.float32)
    # Cosine decay
    # There is no tf.pi so we wrap pi as a TF constant

    learning_rate = 0.5 * target_lr * (1 + tf.cos(tf.constant(3.1415926535897932384626433, dtype=tf.float32) * (global_step - warmup_steps - hold) / (total_steps - warmup_steps - hold)))

    # Target LR * progress of warmup (=1 at the final warmup step)
    warmup_lr = target_lr * (global_step / warmup_steps)

    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.
    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise
    # if hold > 0:
    learning_rate = tf.where(global_step > warmup_steps + hold,
                                learning_rate, target_lr)
    
    learning_rate = tf.where(global_step < warmup_steps, warmup_lr, learning_rate)
    return learning_rate


class WarmUpCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, start_lr, target_lr, warmup_steps, total_steps, hold):
        super().__init__()
        self.start_lr = start_lr
        self.target_lr = target_lr
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.hold = hold

    def __call__(self, step):
        lr = lr_warmup_cosine_decay(global_step=step,
                                    total_steps=self.total_steps,
                                    warmup_steps=self.warmup_steps,
                                    start_lr=self.start_lr,
                                    target_lr=self.target_lr,
                                    hold=self.hold)

        return tf.where(
            step > self.total_steps, float(0.0), lr, name="learning_rate"
        )
    
    def get_config(self):
        config = {
          'start_lr': self.start_lr,
          'target_lr': self.target_lr,
          'warmup_steps': self.warmup_steps,
          'total_steps': self.total_steps,
          'hold': self.hold
        }
        return config
    

def make_model(num_classes=22, num_dense_layers=6, num_conv_layers=5, dense_regularisation=0.000001) -> tf.keras.Model:
    """
    Create the CNN model

    Parameters
    ----------
    num_classes : int, optional
        Number of output classes, by default 22
    num_dense_layers : int, optional
        number of dense layers, by default 6
    num_conv_layers : int, optional
        Number of convolutionnal layers, by default 5
    dense_regularisation : float, optional
        Regularisation factor for dense layers, by default 0.000001

    Returns
    -------
    tf.keras.Model
        Created CNN model
    """

    if dense_regularisation > 0:
        reg_func = tf.keras.regularizers.l1(dense_regularisation)
    else:
        reg_func = None

    model = tf.keras.Sequential()

    for layer in range(num_conv_layers):
        if layer == 0:
            model.add(tf.keras.layers.Conv2D(filters=2**(3+layer),
                                             kernel_size=[6, 2], activation='elu'))
        else:
            model.add(tf.keras.layers.Conv2D(filters=2**(3+layer),
                                             kernel_size=[2, 1], activation='elu'))  # size 4,1
        model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 1], strides=2))

    model.add(tf.keras.layers.Flatten())

    for layer in range(num_dense_layers):
        model.add(tf.keras.layers.Dense(2**(3+num_dense_layers-layer), activation='elu',
                                        kernel_regularizer=reg_func))

    model.add(tf.keras.layers.Dense(num_classes, activation='softmax',
                                    kernel_regularizer=reg_func))

    return model
