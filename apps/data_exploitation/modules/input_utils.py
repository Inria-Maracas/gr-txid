"""
Collection of useful import functions

"""

import glob
import numpy as np
import tensorflow as tf


def find_same_ds_training(data_folder, base_log_folder) -> int:
    """
    Looks at the log folder contents to find how many trainings have been done
    on the specified dataset

    Parameters
    ----------
    data_folder : sring
        Path to the dataset
    base_log_folder : string
        Base logging location

    Returns
    -------
    int
        Current number of same ds training (1+number of found trainings)
    """

    # Concatenates base log, type of scenario, and ds date
    expected_log_path = base_log_folder + \
        data_folder.split('/')[-3] + "/" + data_folder.split('/')[-2]
    print(expected_log_path)
    matching_files = glob.glob(expected_log_path + "/*/")
    print(matching_files)
    # TODO Could change from counting files to finding the biggest number (in case of missing ones)
    return 1 + len(matching_files)


@tf.function(jit_compile=True)
def input_pipeline_mapping(data, label, num_classes=22):
    """
    Concenates the processing steps: turn labels to one-hot, normalise vectors,
    and stack real and imaginary parts

    Parameters
    ----------
    data : 
        Complex vector of 600 elements
    label : int64
        Scalar label
    num_classes : int, optional
        total number of classes (to fix one-hot computation), by default 22

    Returns
    -------
    _type_
        Processed data, one-hot label
    """
    one_hot_lbl = tf.one_hot(label, depth=num_classes, axis=0)
    normed_data = tf.math.l2_normalize(data)
    split_cmplx = tf.reshape(tf.stack((tf.math.real(normed_data),
                                       tf.math.imag(normed_data)), axis=1), (600, 2, 1))

    return split_cmplx, one_hot_lbl


def load_files(file_re_list):
    """
    Load content of list of files

    Parameters
    ----------
    file_re_list : _type_
        List of real part file paths

    Returns
    -------
    _type_
        (array of complex vectors with IQ data, array of labels)
    """
    cmplx_list = []
    tx_id_list = []
    for file_re in file_re_list:
        # Extract tx id from file name
        tx_num = int(file_re.split('/')[-1].split('_')[-1][:-4])

        # Extract rx id from file name, may break if it's not provided
        # rx_num = int(file_re.split('/')[-1].split('_')[-3])

        # print(tx_num)

        file_im = file_re.replace("re_", "im_")
        re = np.fromfile(file_re, dtype=np.float32, count=-1)
        im = np.fromfile(file_im, dtype=np.float32, count=-1)
        cmplx = re + (1j * im)

        if len(cmplx) > 0:  # Avoid tripping on empty files
            # Get integer number of payloads
            cmplx = cmplx[: (len(cmplx)//600) * 600]
            cmplx = np.reshape(cmplx, (-1, 600))
            cmplx_list.append(cmplx)
            tx_id_list.append(
                tx_num * np.ones((cmplx.shape[0], 1), dtype=np.int64))

    # We have a list of complex matrices (one list element per file),
    # with a tx_number to go with each payload
    concat_cmplx = np.concatenate(cmplx_list, axis=0)
    concat_tx_id_list = np.concatenate(tx_id_list, axis=0)

    return concat_cmplx, concat_tx_id_list
