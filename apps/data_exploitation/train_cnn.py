import os
import glob
from argparse import ArgumentParser
import tensorflow as tf
import numpy as np
from modules import model
from modules import input_utils

parser = ArgumentParser()
parser.add_argument("-g", dest="gpu", type=int, default=-1,
                    help="Set to 0 to use the first GPU, 1 for the second, -1 to use ony the CPU")
parser.add_argument("-f", dest="target_folder", type=str,
                    default=".", help="Folder containing dataset to train on")
parser.add_argument("--log-folder", dest="log_folder", type=str,
                    default="/app/data/Tensorboard/TxId/", help="Folder to store logs in")
parser.add_argument("-n", dest="num_classes", type=int,
                    default=22, help="Number of classes")
parser.add_argument("-l", dest="learning_rate", type=float,
                    default=0.001, help="Learning rate")
parser.add_argument("-r", dest="regularisation_rate", type=float,
                    default=0.000001, help="Regularisation parameter")
parser.add_argument("-s", dest="use_scheduler",
                    action='store_true', help="Use cosine scheduler")
parser.add_argument("-b", dest="batch_size", type=int,
                    default=2**8, help="Batch size")
parser.add_argument("-e", dest="epochs", type=int,
                    default=100, help="Number of training epochs")
parser.add_argument("-d", dest="dense_layers", type=int,
                    default=6, help="Number of dense layers")
parser.add_argument("-c", dest="conv_layers", type=int,
                    default=5, help="Number of convolutional layers")
args = parser.parse_args()

os.environ["CUDA_VISIBLE_DEVICES"] = f"{args.gpu}"
# May help with execution speed
os.environ["TF_GPU_THREAD_MODE"] = "gpu_private"

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
print(tf.__version__)


# "/app/local_home/cmorin/Tx-Id/datasets/RobotRx/20230525_16h45/"
data_folder = args.target_folder
base_log_folder = args.log_folder

num_classes = args.num_classes
loss = "categorical_crossentropy"
batch_size = args.batch_size
num_dense_layers = args.dense_layers
num_conv_layers = args.conv_layers
dense_regularisation = args.regularisation_rate

pipeline_parallel_calls = 20  # tf.data.AUTOTUNE

same_ds_training = input_utils.find_same_ds_training(
    data_folder, base_log_folder)

save_log_folder = base_log_folder + \
    data_folder.split('/')[-3] + "/" + \
    data_folder.split('/')[-2]+f"/{same_ds_training:03}"
print(save_log_folder)

# Get list of file names for real components.
# Expects that there is always a matching im file
re_glob = glob.glob(data_folder + "*_re_*.bin")


print(re_glob)
file_amount = len(re_glob)
print(file_amount)

base_ds = tf.data.Dataset.from_tensor_slices(input_utils.load_files(re_glob))
total_packet_num = base_ds.cardinality().numpy()
print(total_packet_num)

shuffled_ds = base_ds.shuffle(
    buffer_size=total_packet_num, reshuffle_each_iteration=False)

train_packet_num = int(0.7 * total_packet_num)
val_packet_num = int(0.1 * total_packet_num)
test_packet_num = int(0.2 * total_packet_num)
print(
    f"Packet number in total {total_packet_num}, train {train_packet_num}, validation {val_packet_num}, and test {test_packet_num}")

train_ds = shuffled_ds.take(train_packet_num)
val_ds = shuffled_ds.skip(train_packet_num).take(val_packet_num)
test_ds = shuffled_ds.skip(train_packet_num+val_packet_num)

splitted_train_ds = train_ds.map(input_utils.input_pipeline_mapping, deterministic=False,
                                 num_parallel_calls=pipeline_parallel_calls).shuffle(buffer_size=batch_size*50)
splitted_val_ds = val_ds.map(input_utils.input_pipeline_mapping,
                             deterministic=False, num_parallel_calls=pipeline_parallel_calls)


if args.use_scheduler:
    # cycle_num = 1
    # t_mul = 2
    # sum_t_mul = 0
    # for i in range(cycle_num):
    #     sum_t_mul += t_mul ** i
    # first_decay_epochs = (args.epochs // sum_t_mul) +1
    # learning_rate = tf.keras.optimizers.schedules.CosineDecayRestarts(args.learning_rate, first_decay_steps=first_decay_epochs*(train_packet_num/batch_size), alpha=0.0001, t_mul=t_mul, m_mul=0.5)


    pw_decay_bounds_epochs_arr = args.epochs * (np.array([0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.4, 0.7, 0.9, 0.95]))
    print(pw_decay_bounds_epochs_arr)
    pw_decay_values = [1e-5, 2e-5, 4e-5, 8e-5, 1e-4, 2e-4, 5e-4, 1e-3, 1e-3, 1e-3, 1e-4, 5e-5, 1e-5]
    learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay(list(pw_decay_bounds_epochs_arr*(train_packet_num/batch_size)), values=pw_decay_values)

else:
    learning_rate = args.learning_rate

model = model.make_model(num_classes=num_classes, num_conv_layers=num_conv_layers,
                         num_dense_layers=num_dense_layers,
                         dense_regularisation=dense_regularisation)

model.compile(optimizer=tf.optimizers.Adam(learning_rate),
              loss=loss, metrics=['categorical_accuracy'], jit_compile=True,)

callbacks = [
    # Interrupt training if `val_loss` stops improving for over 2 epochs
    # tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss', min_delta=0.001),
    # Write TensorBoard logs to `./logs` directory
    tf.keras.callbacks.TensorBoard(log_dir=save_log_folder,
                                   write_graph=False,
                                   update_freq='epoch',
                                   write_steps_per_second=True,
                                   #    profile_batch=(100, 120),
                                   )
]

history = model.fit(splitted_train_ds.batch(batch_size).prefetch(5), epochs=args.epochs,
                    callbacks=callbacks, verbose=2,
                    validation_data=splitted_val_ds.batch(batch_size).prefetch(5))

model.save_weights(
    f"{data_folder}weights/Conv_num={num_conv_layers}_Dens_num={num_dense_layers}_softmax_lr={learning_rate}_batch={batch_size}_regl1={dense_regularisation}_{same_ds_training:03}")
